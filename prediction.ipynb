{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a518a642-7b4e-42c7-b5fa-4fdbee6f5695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2cff0c1-55a8-4392-98d4-0a47901a3c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated synthetic dataset with 1000 rows and saved as 'large_system_usage.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Starting point for timestamp\n",
    "start_time = datetime.strptime(\"Sat Nov 16 10:28:01 UTC 2024\", \"%a %b %d %H:%M:%S %Z %Y\")\n",
    "\n",
    "# Number of rows to generate\n",
    "num_rows = 1000\n",
    "\n",
    "# Generate timestamps\n",
    "timestamps = [start_time + timedelta(minutes=i) for i in range(num_rows)]\n",
    "\n",
    "# Generate synthetic CPU, memory, and disk usage data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "cpu_usage = np.abs(np.random.normal(loc=10, scale=5, size=num_rows))  # Mean 10%, stddev 5%\n",
    "memory_usage = np.random.uniform(low=14.2, high=14.3, size=num_rows)  # Between 14.2 and 14.3\n",
    "disk_usage = np.random.choice([1, 2], size=num_rows, p=[0.8, 0.2])  # Mostly 1, some 2\n",
    "\n",
    "# Create a DataFrame\n",
    "large_data = pd.DataFrame({\n",
    "    \"timestamp\": [t.strftime(\"%a %b %d %H:%M:%S UTC %Y\") for t in timestamps],\n",
    "    \"cpu_usage\": cpu_usage,\n",
    "    \"memory_usage\": memory_usage,\n",
    "    \"disk_usage\": disk_usage\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "large_data.to_csv(\"D:\\AI_powered_system_monitor\\large_system_usage.csv\", index=False)\n",
    "print(\"Generated synthetic dataset with 1000 rows and saved as 'large_system_usage.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb36578-81a4-4875-81b3-443ef31ae9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "                      timestamp  cpu_usage  memory_usage  disk_usage\n",
      "0  Sat Nov 16 10:28:01 UTC 2024  12.483571     14.216748           1\n",
      "1  Sat Nov 16 10:29:01 UTC 2024   9.308678     14.210457           1\n",
      "2  Sat Nov 16 10:30:01 UTC 2024  13.238443     14.263643           1\n",
      "3  Sat Nov 16 10:31:01 UTC 2024  17.615149     14.270648           1\n",
      "4  Sat Nov 16 10:32:01 UTC 2024   8.829233     14.203159           2\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "csv_path=\"D:\\AI_powered_system_monitor\\large_system_usage.csv\"\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "# Display basic info\n",
    "print(\"Dataset Overview:\")\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a147d3a2-2197-4e17-ab7a-d060bedc05fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   timestamp     1000 non-null   object \n",
      " 1   cpu_usage     1000 non-null   float64\n",
      " 2   memory_usage  1000 non-null   float64\n",
      " 3   disk_usage    1000 non-null   int64  \n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 31.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaba589f-6b1d-42b4-b330-8212128b64ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values:\n",
      "timestamp       0\n",
      "cpu_usage       0\n",
      "memory_usage    0\n",
      "disk_usage      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "528e40b1-c0ac-426c-9be7-9e673d4d9ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns in the dataset: ['timestamp', 'cpu_usage', 'memory_usage', 'disk_usage']\n"
     ]
    }
   ],
   "source": [
    "columns = data.columns.tolist()\n",
    "print(f\"\\nColumns in the dataset: {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d29ba09-274b-441e-9f44-e880e1816092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as cpu_usage_predictor.pkl\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "data['Timestamp'] = pd.to_datetime(data['timestamp'], errors='coerce')  # Handles parsing errors\n",
    "data['Seconds'] = (data['Timestamp'] - data['Timestamp'].min()).dt.total_seconds()\n",
    "X = data[['Seconds']]\n",
    "y = data['cpu_usage'].astype(float)  # Target: CPU Usage\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, \"cpu_usage_predictor.pkl\")\n",
    "print(\"Model saved as cpu_usage_predictor.pkl\")\n",
    "\n",
    "\n",
    "# Ensure correct data types (if needed)\n",
    "data['cpu_usage'] = data['cpu_usage'].astype(float)\n",
    "data['memory_usage'] = data['memory_usage'].astype(float)\n",
    "data['disk_usage'] = data['disk_usage'].astype(int)\n",
    "\n",
    "# Feature (X) and Target (y)\n",
    "X = data[['memory_usage', 'disk_usage']]  # Input features (add 'cpu_usage' if you want it as an input instead of target)\n",
    "y = data['cpu_usage']  # Target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save preprocessed data for training\n",
    "X_train.to_csv('X_train.csv', index=False)\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)\n",
    "\n",
    "print(\"\\nData preprocessing completed. Training and testing datasets saved.\")\n",
    "# Load the preprocessed training data\n",
    "X_train = pd.read_csv(\"X_train.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\").squeeze()  # Convert to Series\n",
    "X_test = pd.read_csv(\"X_test.csv\")\n",
    "y_test = pd.read_csv(\"y_test.csv\").squeeze()\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor(n_estimators=200, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, 'cpu_usage_predictor_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
